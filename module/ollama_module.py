import os
import time
import uuid
import base64
import json
import logging
from collections import deque
from typing import Iterator, Dict, Any, Optional, List

import requests

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OllamaClient:
    """Ollama APIì™€ í†µì‹ í•˜ëŠ” ëŒ€í™”í˜• í´ë¼ì´ì–¸íŠ¸."""

    def __init__(
        self,
        model: str = "llama2",
        base_url: str = None,
        generation_config: Optional[Dict[str, Any]] = None,
        rpm_limit: int = 10,
        thinking_config: Any = None,
        **kwargs,
    ):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”."""
        self.base_url = (base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")).rstrip("/")
        self.model = model
        self.history: List[Dict[str, Any]] = []
        self.request_times = deque()
        self.rpm_limit = rpm_limit
        self.generation_config = generation_config.copy() if generation_config else {}

        if thinking_config is not None:
            logger.warning("thinking_config íŒŒë¼ë¯¸í„°ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

        self._init_args = {
            "model": model,
            "base_url": self.base_url,
            "generation_config": self.generation_config.copy(),
            "rpm_limit": rpm_limit,
        }
        self._init_args.update(kwargs)
        logger.info(f"OllamaClient ì´ˆê¸°í™” - ëª¨ë¸: {self.model}, RPM ì œí•œ: {self.rpm_limit}")

    def _check_rpm_limit(self) -> None:
        current_time = time.time()
        while self.request_times and current_time - self.request_times[0] > 60:
            self.request_times.popleft()
        if len(self.request_times) >= self.rpm_limit:
            wait_time = 60 - (current_time - self.request_times[0])
            if wait_time > 0:
                logger.info(f"RPM ì œí•œ ë„ë‹¬. {wait_time:.2f}ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤.")
                time.sleep(wait_time)
                current_time = time.time()
                while self.request_times and current_time - self.request_times[0] > 60:
                    self.request_times.popleft()
        self.request_times.append(time.time())

    def start_chat(self, history: Optional[List[Dict[str, Any]]] = None) -> None:
        self.history = history if history is not None else []
        logger.info("ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    def _prepare_history(self) -> List[Dict[str, Any]]:
        processed = []
        for message in self.history:
            msg = {"role": message.get("role", "user"), "content": message.get("content", "")}
            if message.get("images"):
                base64_images = []
                for path in message["images"]:
                    try:
                        with open(path, "rb") as f:
                            base64_images.append(base64.b64encode(f.read()).decode("utf-8"))
                    except Exception as e:
                        logger.error(f"ì´ë¯¸ì§€ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
                if base64_images:
                    msg["images"] = base64_images
            processed.append(msg)
        return processed

    def send_message(
        self,
        message: str,
        model: str = None,
        images: Optional[List[str]] = None,
        **kwargs,
    ) -> str:
        # ì¶”ê°€ ì¸ìëŠ” ë¬´ì‹œí•˜ì§€ë§Œ, í˜¸í™˜ì„±ì„ ìœ„í•´ ê²½ê³  ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
        if kwargs:
            logger.debug(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì¶”ê°€ ì¸ì ë¬´ì‹œ: {list(kwargs.keys())}")

        self._check_rpm_limit()
        user_msg = {"role": "user", "content": message}
        if images:
            user_msg["images"] = images
        self.history.append(user_msg)
        payload = {
            "model": model or self.model,
            "messages": self._prepare_history(),
            "stream": False,
        }
        try:
            response = requests.post(f"{self.base_url}/api/chat", json=payload)
            response.raise_for_status()
            data = response.json()
            text = data.get("message", {}).get("content", "")
            self.history.append({"role": "assistant", "content": text})
            return text
        except requests.exceptions.RequestException as e:
            self.history.pop()
            logger.error(f"Ollama API ì˜¤ë¥˜: {e}")
            raise

    def send_message_stream(
        self,
        message: str,
        model: str = None,
        images: Optional[List[str]] = None,
        **kwargs,
    ) -> Iterator[str]:
        # ì¶”ê°€ ì¸ìëŠ” ë¬´ì‹œí•˜ì§€ë§Œ, í˜¸í™˜ì„±ì„ ìœ„í•´ ê²½ê³  ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
        if kwargs:
            logger.debug(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì¶”ê°€ ì¸ì ë¬´ì‹œ: {list(kwargs.keys())}")

        self._check_rpm_limit()
        user_msg = {"role": "user", "content": message}
        if images:
            user_msg["images"] = images
        self.history.append(user_msg)
        payload = {
            "model": model or self.model,
            "messages": self._prepare_history(),
            "stream": True,
        }
        try:
            resp = requests.post(f"{self.base_url}/api/chat", json=payload, stream=True)
            resp.raise_for_status()
            full_response = []
            for line in resp.iter_lines():
                if line:
                    decoded = line.decode("utf-8")
                    try:
                        data = json.loads(decoded)
                        if data.get("message") and data["message"].get("content"):
                            full_response.append(data["message"]["content"])
                        yield decoded + "\n"
                    except json.JSONDecodeError:
                        yield json.dumps({"error": "ì‘ë‹µ ë””ì½”ë”© ì˜¤ë¥˜"}) + "\n"
            if full_response:
                self.history.append({"role": "assistant", "content": "".join(full_response)})
        except requests.exceptions.RequestException as e:
            self.history.pop()
            logger.error(f"Ollama API ì˜¤ë¥˜: {e}")
            raise

    def delete_last_turn(self) -> bool:
        if len(self.history) < 2:
            logger.warning("ì‚­ì œí•  ëŒ€í™” í„´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        self.history.pop()
        self.history.pop()
        return True

    def fork(self) -> "OllamaClient":
        client = OllamaClient(**self._init_args)
        client.start_chat(history=[msg.copy() for msg in self.history])
        return client

    def modify_message(self, index: int, new_message: str) -> Optional["OllamaClient"]:
        if not (0 <= index < len(self.history)):
            logger.error(f"ì˜ëª»ëœ ì¸ë±ìŠ¤: {index}")
            return None
        if self.history[index].get("role") != "user":
            logger.error("ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return None
        modified = [msg.copy() for msg in self.history[: index + 1]]
        modified[index]["content"] = new_message
        new_client = OllamaClient(**self._init_args)
        new_client.start_chat(history=modified)
        return new_client

    def get_history(self) -> List[Dict[str, Any]]:
        return self.history.copy()

    def set_rpm_limit(self, rpm_limit: int) -> None:
        if rpm_limit <= 0:
            raise ValueError("RPM ì œí•œì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")
        self.rpm_limit = rpm_limit
        self._init_args["rpm_limit"] = rpm_limit

    def get_current_rpm_usage(self) -> Dict[str, Any]:
        current_time = time.time()
        while self.request_times and current_time - self.request_times[0] > 60:
            self.request_times.popleft()
        current_count = len(self.request_times)
        time_until_reset = 60 - (current_time - self.request_times[0]) if self.request_times else 0
        return {
            "limit": self.rpm_limit,
            "current_count": current_count,
            "remaining": max(0, self.rpm_limit - current_count),
            "time_until_reset": max(0, time_until_reset),
        }

    def get_config(self) -> Dict[str, Any]:
        return {
            "model": self.model,
            "generation_config": self.generation_config.copy(),
            "rpm_limit": self.rpm_limit,
        }

    def update_generation_config(self, **kwargs) -> None:
        self.generation_config.update(kwargs)
        self._init_args["generation_config"] = self.generation_config.copy()

    def set_thinking_budget(self, budget: int) -> None:
        logger.warning("thinking_budget ì„¤ì •ì€ Ollamaì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    def get_thinking_budget(self) -> int:
        logger.warning("thinking_budget ì¡°íšŒëŠ” Ollamaì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return 0

    def set_response_schema(
        self, response_schema: Optional[Dict] = None, response_mime_type: str = None
    ) -> None:
        logger.warning("OllamaëŠ” response_schema ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


__all__ = ["OllamaClient"]

if __name__ == "__main__":
    # OllamaClient í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import sys
    
    def test_ollama_client():
        """OllamaClientì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print("=== OllamaClient í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        try:
            # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
            print("\n1. í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
            client = OllamaClient(
                model="qwen3:8b-q8_0",
                rpm_limit=2
            )
            print(f"âœ“ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ - ëª¨ë¸: {client.model}")
            
            # ì„¤ì • í™•ì¸ í…ŒìŠ¤íŠ¸
            print("\n2. ì„¤ì • í™•ì¸ í…ŒìŠ¤íŠ¸")
            config = client.get_config()
            print(f"âœ“ í˜„ì¬ ì„¤ì •: {config}")
            
            # RPM ì œí•œ í…ŒìŠ¤íŠ¸
            print("\n3. RPM ì œí•œ í…ŒìŠ¤íŠ¸")
            rpm_status = client.get_current_rpm_usage()
            print(f"âœ“ RPM ìƒíƒœ: {rpm_status}")
            
            # ëŒ€í™” ì‹œì‘ í…ŒìŠ¤íŠ¸
            print("\n4. ëŒ€í™” ì‹œì‘ í…ŒìŠ¤íŠ¸")
            client.start_chat()
            print("âœ“ ëŒ€í™” ì‹œì‘ ì„±ê³µ")
            
            # ê°„ë‹¨í•œ ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš°ì—ë§Œ)
            print("\n5. ë©”ì‹œì§€ ì „ì†¡ í…ŒìŠ¤íŠ¸")
            try:
                response = client.send_message("ì•ˆë…•í•˜ì„¸ìš”! ê°„ë‹¨í•œ ì¸ì‚¬ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
                print(f"âœ“ ì‘ë‹µ ë°›ìŒ: {response[:100]}...")
            except Exception as e:
                print(f"âš  ë©”ì‹œì§€ ì „ì†¡ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤): {e}")
            
            # íˆìŠ¤í† ë¦¬ í…ŒìŠ¤íŠ¸
            print("\n6. íˆìŠ¤í† ë¦¬ í…ŒìŠ¤íŠ¸")
            history = client.get_history()
            print(f"âœ“ í˜„ì¬ íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(history)}")
            
            # ë³µì œ í…ŒìŠ¤íŠ¸
            print("\n7. í´ë¼ì´ì–¸íŠ¸ ë³µì œ í…ŒìŠ¤íŠ¸")
            cloned_client = client.clone()
            print("âœ“ í´ë¼ì´ì–¸íŠ¸ ë³µì œ ì„±ê³µ")
            
            # thinking budget í…ŒìŠ¤íŠ¸ (ê²½ê³  ë©”ì‹œì§€ í™•ì¸)
            print("\n8. Thinking Budget í…ŒìŠ¤íŠ¸")
            client.set_thinking_budget(1000)
            budget = client.get_thinking_budget()
            print(f"âœ“ Thinking Budget: {budget}")
            
            # response schema í…ŒìŠ¤íŠ¸ (ê²½ê³  ë©”ì‹œì§€ í™•ì¸)
            print("\n9. Response Schema í…ŒìŠ¤íŠ¸")
            test_schema = {
                "type": "object",
                "properties": {
                    "message": {"type": "string"}
                }
            }
            client.set_response_schema(test_schema)
            print("âœ“ Response Schema ì„¤ì • ì™„ë£Œ")
            
            print("\n=== ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
            
        except Exception as e:
            print(f"âœ— í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
        
        return True
    
    def test_error_handling():
        """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("\n=== ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ì˜ëª»ëœ RPM ì œí•œ ì„¤ì • í…ŒìŠ¤íŠ¸
            print("1. ì˜ëª»ëœ RPM ì œí•œ ì„¤ì • í…ŒìŠ¤íŠ¸")
            client = OllamaClient()
            try:
                client.set_rpm_limit(0)
                print("âœ— ì˜ˆìƒëœ ValueErrorê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except ValueError:
                print("âœ“ ValueError ì •ìƒì ìœ¼ë¡œ ë°œìƒ")
            
            # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì„œë²„ í…ŒìŠ¤íŠ¸
            print("\n2. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì„œë²„ í…ŒìŠ¤íŠ¸")
            client_invalid = OllamaClient(base_url="http://localhost:99999")
            try:
                client_invalid.send_message("í…ŒìŠ¤íŠ¸")
                print("âœ— ì—°ê²° ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            except Exception:
                print("âœ“ ì—°ê²° ì—ëŸ¬ ì •ìƒì ìœ¼ë¡œ ì²˜ë¦¬ë¨")
            
            print("\n=== ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
            
        except Exception as e:
            print(f"âœ— ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
        
        return True
    
    # ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("OllamaClient í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    success = True
    success &= test_ollama_client()
    success &= test_error_handling()
    
    if success:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        sys.exit(0)
    else:
        print("\nâŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

